9

since the 1980s, successive governments and big business have worked to promote a new vision of mental health; one that puts at its centre a new kind of person: resilient, optimistic, individualistic and above all, economically productive – the kind of person the new economy needs and wants. As a result of this shift, our entire approach to mental health has radically altered to meet these market demands. We define ‘return to health’ as a ‘return to work’. We blame suffering on faulty minds and brains rather than on harmful social, political and work environments. We promote highly profitable drug interventions, which, if great news for big pharmaceutical corporations, are in the long term holding millions of people back.

9

this marketised vision of mental health has stripped our suffering of its deeper meaning and purpose. Consequently, our distress is no longer seen as a vital call to change or as anything potentially transformative or instructive. It has rather become, over the last few decades, an occasion for yet more buying and selling. Whole industries have thrived on the basis of this logic, offering self-interested explanations and solutions for the many pains of living.

10

since the 1980s, this pro-market agenda has begun to harm both the UK and the West in general, turning our entire approach to mental health into something preoccupied with sedating us, depoliticising our discontent and keeping us productive and subservient to the economic status quo. By putting economic servitude before real individual health and flourishing, our priorities have become dramatically and dangerously misplaced, and more suffering, paradoxically, has been the unhappy result.

11

these problems did not emerge in a vacuum, but have thrived under the new style of capitalism that has governed us since the 1980s, one favouring a particular type of thinking about mental health and intervention; one that has put the needs of the economy before our own, while anaesthetising us to the often psychosocial roots of our despair. As a result, we are rapidly becoming a nation sedated by mental health interventions that greatly overplay the help they bring; that subtly teach us to accept and endure, rather than to stand up and challenge, the social and relational conditions harming us and holding us back.

12

A central criticism of this sprawling ‘book of woe’ is that since the 1980s it has unjustifiably expanded the definition of mental illness to encompass more and more domains of human experience. It achieved this by rapidly increasing the number of mental disorders believed to exist (from 106 in the early 1970s to around 370 today), and by progressively lowering the bar for what constitutes a psychiatric disorder (making it easier for any of us to be classed as ‘mentally ill’).7 These processes resulted in much of our everyday human distress being wrongly medicalised, pathologised and eventually medicated. Grief at a significant loss, struggling to reach orgasm, experiencing lack of concentration at school, undergoing trauma, feeling anxious about public events or simply underperforming at work are just some of the manifold painful human experiences that the DSM has medically rebranded as symptoms of psychiatric illness.

13

What strengthened the international critique was that this expansion occurred without any real biological justification. Unlike for most physical illnesses in general medicine (e.g. heart disease, cancer and infectious diseases), no biological causes have been found for the vast majority of mental disorders in the DSM. This explains why there are still no blood or urine tests, no scans, X-rays or other objective assessments that can verify any psychiatric diagnosis. There are simply no discovered biological abnormalities for which to test. Psychiatric labels, in other words, do not correspond to known biological pathologies that treatments can then target and ‘cure’. They are rather socially constructed labels ascribed to collections of feelings and behaviours deemed disordered or pathological by the psychiatric committees who compiled the DSM.

14

The fact that these agreements were mostly made in the face of weak and contradictory evidence has long been a bone of contention in the mental health community. As one of the most important figures in the seminal edition of the DSM (DSM-III) summarised rather well: ‘There was very little systematic research \[guiding the creation of the DSM\], and much of the research that existed was really a hodgepodge – scattered, inconsistent, and ambiguous. I think the majority of us recognised that the amount of good, solid science upon which we were making our decisions was pretty modest.’9

Given that the evidence base was scattered and ambiguous, how was DSM committee agreement eventually reached? According to the archival and interview data regarding the most important modern edition,10 it was mostly done by way of committee vote. One leading member of the DSM-III committee described to me a typical voting process: ‘Some things were discussed over a number of different meetings, \[which would sometimes be\] followed by an exchange of memoranda about it, and then there would simply be a vote … people would raise hands, there weren’t that many people.’ Another said: ‘We had very little in the way of data, so we were forced to rely on clinical consensus, which, admittedly, is a very poor way to do things. But it was better than anything else we had … If people were divided, the matter would be eventually decided by a vote.’11

The DSM’s categorisation of diverse human experiences into approximately 370 separate psychiatric disorders was not, then, the outcome of solid neurobiological research. It was mostly based on vote-based judgements reached by small, select groups of DSM psychiatrists – judgements then ratified and seemingly scientifically legitimised by their inclusion in the manual.

16

Indeed, as the most important chairperson in the history of the DSM, Robert Spitzer, later acknowledged: ‘the pharmaceuticals were delighted’ with the manual’s widespread medicalisation of distress, as it created a vast and highly profitable market for their products.13

The above claim about pharma’s distribution of DSM14 is perfectly consistent with what we have come to learn about the tactics drugs companies have deployed over the last thirty years to aggressively promote psychiatric drugs, on both sides of the Atlantic. The truth is, since the 1990s, the pharmaceutical industry has been a major financial sponsor of UK and US academic psychiatry, significantly shaping psychiatric research, training and practice within the field.15 It has also opaquely funded many influential mental health charities, patient groups, heads of psychiatry departments,16 as well as leading professional psychiatric organisations – including, naturally, the publisher of the DSM.17

308

15 Carlat, D. (2010), Unhinged: the trouble with psychiatry, London: Free Press. Gøtzsche, P. (2013), Deadly Medicines and Organised Crime: how Big Pharma has corrupted healthcare, London: Radcliffe Publishing. Whitaker, R., and Cosgrove, L. (2015), Psychiatry Under the Influence: institutional corruption, social injury, and prescriptions for reform, New York: Palgrave Macmillan.

308

16 Campbell, E. G., et al. (2007), ‘Institutional academic-industry relationships’, Journal of the American Medical Association 298 (15):1779–80.

308

17 Whitaker, R. (2017), ‘Psychiatry Under the Influence’, in Davies, J. (ed), The Sedated Society: the causes and harms of our psychiatric drug epidemic, London: Palgrave Macmillan.

18

Furthermore, the industry has paid for, commissioned, designed and conducted nearly all the clinical trials into psychiatric drugs (antidepressants, antipsychotics, tranquillisers).18 This has enabled companies to literally create an evidence base in their favour, often by way of dubious research practices designed to legitimise their products.19 These include burying negative data; ghostwriting academic articles; manipulating outcomes to boost the appearance of effectiveness; hiding inconvenient harms; enticing journals and editors with financial incentives, and concealing bad science behind slick and deceptive medical marketing campaigns.20 We also know, through countless academic studies, how most leading psychiatric drug researchers have received industry money (i.e. funding, consultancy fees, speaker’s fees or other honoraria), and how such financial entanglements exert demonstrable biasing effects.21 This is to say that clinicians, researchers, organisations and DSM committee members who receive industry money are far more likely to promote and advocate drug company products in their research, clinical practices, teaching and public statements than those without such financial links. Given that these links have literally littered the profession over the last thirty years, it is little wonder that the over-medicalisation and medicating of emotional distress has similarly proliferated.22

308

18 Angell, M. (2011), ‘The illusions of psychiatry’, New York Review of Books 58 (12):82–4.

309

19 Spielmans, G. I., and Parry, P. I. (2010), ‘From evidence-based medicine to marketing-based medicine: evidence from internal industry documents’, Bioethical Inquiry 7: 13–29. Turner, E. H., et al. (2008), ‘Selective publication of antidepressant trials and its influence on apparent efficacy’, New England Journal of Medicine 17:252–60. Kondro, W., and Sibbald, B. (2004), ‘Drug company experts advised to withhold data about SSRI use in children’, Canadian Medical Association Journal 170:783.

309

20 Spielmans, G. I., and Parry, P. I. (2010), ‘From evidence-based medicine to marketing-based medicine: evidence from internal industry documents’, Bioethical Inquiry 7: 13–29. Turner, E. H., et al. (2008), ‘Selective publication of antidepressant trials and its influence on apparent efficacy’, New England Journal of Medicine 17:252–60.

22

for many people, being medicalised can also negatively influence how others treat and perceive those who have been diagnosed. We know, for example, that framing emotional problems in terms of an illness or disorder is more likely to kindle fear, suspicion and hostility in other people than if we articulate those very same problems in non-medical, psychological terms.43

313

43 See: Timimi, S. (2011), ‘Campaign to Abolish Psychiatric Diagnostic Systems such as ICD and DSM’, http://www.criticalpsychiatry.co.uk/index.php?option=com_content&view=article&id=233:campaign-to-abolish-psychiatric-diagnostic-systems-such-as-icd-anddsm-timimi-s&catid=34:members-publications&Itemid=56.

23

Similar forms of stigma even exist when people are ascribed the least stigmatising labels, such as depression. For example, recipients so labelled are still more likely than non-recipients to be viewed by others as having frail wills or character flaws, as being afflicted by personal weakness, or as being lazy and unpredictable.45 And when people are ascribed with more serious labels, like schizophrenia, they are more likely to be perceived as highly unpredictable and potentially dangerous, which can compound their sense of isolation through social rejection.46 In fact, even when people are given false diagnoses by researchers, members of the public will still stigmatise the behaviour of these patients, despite such patients behaving completely normally. The labels, in other words, have powerful cultural effects that shape public perceptions of those being diagnosed, even if these negative perceptions bear no relation to the person at all.

314

45 Corrigan, P. W., and Watson, A. C. (2002), ‘Understanding the impact of stigma on people with mental illness’, World Psychiatry 1(1): 16–20.

314

46 Kvaalea, Erlend P., et al. (2013), ‘Biogenetic explanations and stigma: A meta-analytic review of associations among laypeople’, Social Science & Medicine 96:95–103, https://doi.org/10.1016/j.socscimed.2013.07.017. Mehta, S. and Farina, A. (1997), ‘Is being “sick” really better? Effect of the disease view of mental disorder on stigma’, Journal of Social and Clinical Psychology 16:405–19, 10.1521/jscp.1997.16.4.405.

31

By rejecting the 1970s, then, Thatcher was also rejecting an entire economic and social model that had brought high and sustained levels of economic prosperity throughout the 1950s, 1960s and part of the 1970s. From now on, that old paradigm – regulated capitalism – would be superseded by a new economic order: a new capitalism, a neo-liberalism, increasing the role of market forces in society and encouraging the kinds of personal qualities – competitiveness, self-reliance, entrepreneurialism and productivity – esteemed by Thatcher’s political elite.

33

Friedrich Engels and Karl Marx.

Both men believed that the exploitation of factory workers could only succeed if the workers themselves accepted their own oppression to be both natural and inevitable. What concerned them was the extent to which this acceptance had become deeply engrained in the workers they observed, keeping them in a state of servitude (which compounded their oppression) and a state of isolation from each other (which inhibited their working together for constructive change). Oppressive working conditions had dehumanised people to the extent that they had become detached or alienated from their essential human rights and needs, leaving them in a state of moral and political limbo. And in this demoralised and politically apathetic state, all that remained for them were soothing illusions and anaesthetics – sedatives to compensate them for the painful oppression they endured. Before Marx and Engels met in that café, Marx had already set about

34

identifying one such powerful sedative: organised religion.

Marx felt that religion, unknown to itself, was helping to support the exploitation of factory workers by sedating them to the very suffering that, if fully experienced, would lead them to unite to fight for reform. His view was based on the idea that suffering had always been a powerful driver of social reform: once people experienced the full force of their own despair, they would be compelled to identify and overthrow its causes. Religion, he believed, was interfering with this natural process by recasting the suffering of workers not as a legitimate response to their oppressed situation, but as a ‘hallmark of the pious life’; a godly experience that, if simply endured in this life, would be handsomely rewarded in the next.

Marx believed that by making a religious virtue of suffering, Christianity was indirectly teaching people to accept and endure rather than fight and reform the oppressive conditions harming them.5 Just like any other sedative, religion could offer temporary respite from harsh social and economic realities. But in the long run it would end up causing greater harm, suppressing the human instinct for social reform and allowing harmful circumstances and institutions to live on. It was in this sense that Marx characterised religion as the real opium of the masses, as it sedated the drive for necessary social transformation.

36

What Marx had in effect discovered through his analysis of religion was that those social institutions responsible for understanding and managing suffering were critically important to the aims of an economy. They had the power to defuse politically dangerous emotions by sedating people to the true origins of their distress (cutting off the route to finding the right social solutions).

36

As this understanding unhitched itself from Marxism, and became part of mainstream social science, it started to be applied to the domain of mental health, with many new insights emerging from the 1980s onwards. These revealed the precise ways in which our distress was being misread, exploited and depoliticised for clear economic ends. If I were to draw up a list of how this works, it would look something like this:

36

• Conceptualise human suffering in ways that protect the current economy from criticism. That is, reframe suffering as being rooted in individual rather than social causes, leading individuals to think that it is them rather than the economic and social system in which they live that is at fault and in need of reform.

37

• Redefine individual well-being in terms consistent with the goals of the economy. Well-being should be characterised as comprising those feelings, values and behaviours (e.g. personal ambition, competition and industrious endeavour) that serve economic growth and increased productivity, irrespective of whether they are actually good for the individual and the community.

37

• Turn behaviours and emotions that might negatively impact the economy into a call for more medical intervention. Behaviours and feelings that perturb or disrupt the established order (e.g. low worker satisfaction) should be medicalised and treated, as these can frustrate the economic interests of powerful financial institutions and elites.

37

• Turn suffering into a vibrant market opportunity for more consumption. Suffering should become highly lucrative to big business as it begins to manufacture and market its so-called solutions – solutions from which increased tax revenues, profits and higher share value can be extracted.

38

Now, while it is tempting to dismiss the above devices on the grounds that they all sound a little too conspiratorial, it is important to understand that those who exposed them never claimed that they were deliberately concocted in small smoky rooms with calculated intent.8 Their point was far subtler than that: if any institution is to thrive, it must broadly adapt to what its society wants. And so, in the case of mental health, these strategies arose spontaneously as the sector struggled to endure under a new set of economic arrangements. The embracing of a mental health ideology favourable to the wider economy would not just reconfigure the entire mental health enterprise, but would increasingly help alter the psychological outlook of a whole generation. In this sense, Margaret Thatcher was correct when she said that if you wanted to change the heart and soul of a nation, you must change the whole economic approach, as this is the surest mechanism for influencing in powerful ways the direction in which people and institutions ultimately strive.

39

2

THE NEW CULTURE OF PROLIFERATING DEBT AND DRUGS

40

The history of why consumer debt became such a crucial cog in the modern economic machine is an interesting one. Very briefly put, we can trace it to the idea that rising inflation in the 1970s was mostly due to unions demanding excessive wages for workers. This belief led Thatcher’s government to dismantle union power, which saw employers reap the dividends, as they were able to pay their workers less. This fall in wages was further compounded by the importation of new machine technologies that replaced much human labour, and by the outsourcing of local jobs to cheaper workforces abroad. As these changes rendered many previous jobs redundant, competition in the employment market rose rapidly, which naturally depressed wages even further.

40

While all these changes helped bring wages and inflation down, they also introduced deeper structural problems into the economy. Lower wages meant that people had less disposable income in their pockets to spend on goods and services, which of course would adversely impact consumption and profits. This in turn would threaten investment, potentially creating a cycle of further job losses, lower consumption and yet more contractions in the economy. Fear of this spiral downwards explains why successive governments made credit so accessible throughout the 1980s, 1990s and onwards: debt enabled consumer spending to remain active even when wages were flatlining. In this sense, debt became the sticking plaster covering a deeper structural infection. (See Figure 1.)

40

Figure 1 Rising UK household debt3

41

The use of personal debt as a kind of economic Band-Aid to shield us from deeper structural problems in the economy did not only generate significant societal effects, it was also responsible for shaping the dynamics of our individual selves, or, as Margaret Thatcher put it, the very ‘hearts and souls’ of those becoming increasingly indebted. Up until the 1970s, having personal debt beyond a mortgage held a certain stigma. If you took on any debt at all, it had to be for investment purposes. Other forms of debt (to consume, to ‘keep up with the Joneses’ or to ‘make ends meet’) were largely considered off-limits. A combination of tight cultural mores and credit regulations therefore kept household debt low throughout the 1950s, 1960s and 1970s.

But owing to the deregulatory and structural changes to the economy outlined above, public attitudes soon liberalised, making the adoption of debt almost essential to modern living. From the 1980s onwards, as borrowing became more ubiquitous, young adults in their twenties and thirties were 50 per cent more likely to take on debt than their equivalents living in the three previous decades.4 Being indebted was quickly being normalised. And with this, the very psychological outlook of those indebted began changing too. Owning increasing amounts of debt was altering the complexion of how people envisaged themselves and their future, creating mass shifts in attitude and behaviour not seen before.

To understand this, consider for a moment what happens – psychologically speaking – when we take on debt. When we use that money to satisfy a particular current need, we immediately become shackled to its repayment, and this will impact our choices regarding the future. This is what led the social critic Noam Chomsky to argue that student debt was ultimately detrimental. Taking on excessive debts at a young age limits students’ horizons at a time when they should be at their most expansive. As the reality of borrowing large amounts of money looms, more students are forced to think pragmatically, fiscally, electing to study safe subjects they feel will lead to the highest-paid jobs. This explains why as student fees go up, applications for creative and humanities subjects go down.5 It also explains why modern students are more likely to become fiscally conservative early on, as they’ve been socialised by debt into deferring to a life of repayment. For Chomsky, debt renders students conformists in the economy, forcing them to accept rather than oppose the economic realities of the system they are entering. Debt, in other words, is a potent form of socialisation into neo-liberalism – forcing young people to submit, early on, to the economic status quo.

43

Underpinning critiques like the above is the view that indebtedness not only alters how we think and act, but impinges upon our freedom. Having to repay debt reduces our options, trapping us in future obligations and activities we never directly sought. From this standpoint, the common phrase ‘to borrow against my future self’ actually means ‘to borrow against my future freedom’. What I borrow today frees me now, but ensnares me tomorrow. This is why assuming debt feels easier than it should. In the short term, only its benefits are felt, while entrapment is deferred for another day. Beyond debt for rational investment (and student debt, admittedly, can potentially take this form), most types of debt are not about investment at all, but about operating rationally in a consumerist economy, trying to make ends meet or in some cases trying to survive.

43

While the effect of indebtedness, both societal and personal, has received extensive coverage since the 2008 recession, the plain truth is that debt is just one example of how many deeper problems in the economy have been managed with sticking-plaster solutions. Soon other economic Band-Aids would surface from the 1980s onwards, and not all in the realm of wages, taxation, debt and consumption. We would see economic sticking plasters being deployed throughout our public services: in education, in local authorities, in the NHS and, of course, in the domain of mental health. Here sticking plasters analogous to the usage of consumer debt would rapidly proliferate in unexpected ways, often in the name of spreading safe, effective and useful treatments. And this is, you may have guessed, where antidepressants come in.

44

In 2007, the Pulitzer Prize finalist and former fellow at Harvard University’s Safra Center, Robert Whitaker, encountered a newly published research paper that would alter the course of his professional life. Although the paper had generated little attention at the time, Whitaker believed that its findings threatened to turn conventional clinical wisdom on its head. In essence, it showed that many psychiatric drugs, when consumed long term, were harmful to a large number of the people they purported to help.6

46

Now an obvious response to this study is as follows: surely those with milder symptoms simply self-selected to stop their medication, whereas those with more severe problems elected to stay on the drugs. Perhaps this can explain why those who remained on medication ended up doing far worse in the long term: they were more severely distressed in the first place. And indeed this critique would have been fatal for Harrow’s work had his data not also revealed something alarming: that on aggregate, the more severely ill people who stopped the drugs did better than the less severely ill who stayed on them.7 In other words, his results starkly contradicted the widespread belief in psychiatry that most people with major mental disorder should stay on these drugs for life.

316

7 For discussion on this point see Whitaker’s defence: Whitaker, R. (2016), ‘The Evidence-Based Mind of Psychiatry on Display’, https://www.madinamerica.com/2016/05/the-evidence-based-mind-of-psychiatry-on-display/ (accessed Oct. 2020).

66

A recent article in the British Journal of General Practice has shed some light on why people seem unable to come off medications like antidepressants when they should be withdrawing from them. It discusses how patients (and, surprisingly, also their GPs) are often deeply apprehensive about what effects stopping will have. This fear is due to certain misleading myths about antidepressants and depression that have been exported from psychiatry to primary care over the last three decades. These myths include the false and unsubstantiated idea that major depression is often a chronic (lifelong) condition requiring continuous treatment, and that any improvement while on the drugs is mostly due to the drugs themselves rather than to other factors (such as situational change, placebo effects, or the natural course of depression coming to an end). For those subscribing to these myths, stopping antidepressants is often advised against, because, as the doctors put it, ‘Why disturb the patient’s equilibrium’?41

319

41 Eveleigh, R., et al. (2018), ‘Withdrawal of unnecessary antidepressant medication: a randomised controlled trial in primary care’, BJGP Open 1 (4):bjgpopen17X101265, doi.org/10.3399/bjgpopen17X101265.

67

If the above myths help propel unnecessary long-term antidepressant consumption, they nevertheless pale in comparison to the next driving factor. This is the dominant myth that any withdrawal effects that accompany stopping antidepressants are invariably mild and short-lived (usually lasting for only 1–2 weeks). The origins of this myth can be traced to a symposium held in 1996, which was funded by the drug company Eli Lilly, the maker of the antidepressant Prozac. Here a committee of company-funded psychiatrists reached the consensus that antidepressant withdrawal was largely a minor affair, resolving over about a week. From here, this myth eventually made its way into clinical guidelines internationally, despite it enjoying no real corroborating research. As it took root in clinical practice, doctors who then encountered severe or protracted withdrawal reactions would often deny these reactions, rather believing that their patients were relapsing into their original problems. As a consequence, drugs would often be reinstated and longer-term use would ensue. This dynamic may partly explain why, since the guidelines were issued in 2004, the length of time the average person in the UK spends on an antidepressant has doubled.

68

Two years ago, this myth was finally debunked when research emerged that showed antidepressant withdrawal to be, on aggregate, more severe, protracted and common than clinical guidelines acknowledged.42 It revealed that around 50 per cent of antidepressant users experienced withdrawal, with up to half of those reporting their withdrawal as severe, and that a significant proportion experienced withdrawal for many weeks, months or beyond. In 2019, these findings finally led to the UK’s national clinical antidepressant guidelines being revised, a revision later adopted, in a major U-turn, by the Royal College of Psychiatrists. Now that severe and protracted withdrawal is formally recognised, the hope is that long-term prescribing will consequently reduce, as withdrawal is less regularly misread as relapse and drugs are less unnecessarily reinstated.

320

42 In 2018, Prof. John Read and I challenged this myth in a systematic review published in the journal Addictive Behaviours, which we later summarised in the British Medical Journal. When we published our research, it gained wide media coverage, creating substantial public and professional controversy, with rebuttals and counter-rebuttals going back and forth. Eventually, other research supportive of our position was published in The Lancet, and from that a professional consensus began to build. This ultimately led to the Royal College of Psychiatrists and NICE changing their national guidance, in line with the conclusions of our research.

69

False myths are not the only factors fuelling our long-term prescribing epidemic. Despite the misdiagnosis of withdrawal problems and people’s fear about stopping the drugs, many people take or continue to take them simply because there are so few alternatives on offer. In England last year, 7.4 million adults were prescribed an antidepressant in the NHS, compared to only 1 million who were referred for a psychological therapy. And this is not because people prefer the drugs; the majority of people consulting a GP for help would prefer a talking therapy43 or some form of social support. It is rather because our services are bereft of psychosocial alternatives, leaving 1 in 10 people having to wait over a year to access NHS therapy (and the rest often waiting for many weeks or months).44 Thus drugs have become the overwhelmingly prevalent mental health intervention not because of their high safety, efficacy and desirability, but due to decades of chronic underfunding of services, the dominance of powerful pharmaceutical/psychiatric interests, and a drug-first approach that has neatly aligned with the preferences of late capitalism (as we will later explore). In this sense, they have thrived not because they have improved the emotional life of the nation (the opposite, in fact, is more likely to be true), but because they have served as a sticking plaster for deeper maladies caused by structural problems afflicting our mental health sector and wider economy.

320

43 McHugh, R. K., et al. (2013), ‘Patient preference for psychological vs pharmacologic treatment of psychiatric disorders: a meta-analytic review’, Journal of Clinical Psychiatry 74 (6):595–602, PMID: 23842011.

320

44 Mind (2013), ‘We still need to talk: a report on access to talking therapies’, https://www.mind.org.uk/media/494424/we-still-need-to-talk_report.pdf (accessed Mar. 2018).

70

I began this chapter with a discussion of household debt. Perhaps you may now begin to understand why. There is something oddly analogous about how debt and drugs have operated socially since the 1980s. While the use of both was modest in the 1970s, their consumption has exploded in the decades since. And although there are obviously rational uses for both (debt for sensible investment, and some drug use for short-term stabilisation of the most severe forms of distress), the vast proportion of both household debt and drug consumption appears harmful in the long run. The reason why the consumption of both is at an all-time high, therefore, has nothing to do with improving people’s lives, at least not in any deep and sustainable sense. It is rather a response to profound structural problems in our society that debt and drugs have sought to mask. In this sense, both have become definitive sedatives of our times, which, as work like Robert Whitaker’s reminds us, may ultimately create more problems than they solve.

70

But this is not where their similarities end. Aside from pouring vast wealth into the hands of large corporations, debt and drugs have also both acted ideologically, by reclassifying social problems as individual/internal handicaps that their products can purportedly remedy. Psychiatric interventions are framed as targeting biological irregularities supposedly sitting at the base of our ailing mental health, while debt interventions are framed as targeting the economic inadequacies supposedly driving our ailing financial health. Each debt or drug intervention, by claiming to remedy so-called individual deficits, also subtly acquits new capitalist ideas, institutions and policies of any causal responsibility.

72

3

THE NEW DISSATISFACTIONS OF MODERN WORK

73

The national lurch towards the service sector brought profound changes to the working lives of the UK population, placing on it new and unique demands. Some of these demands were obvious from the start, as they could be easily measured. Firstly, from the 2000s, the average number of hours people worked began to rise considerably, as overtime became almost obligatory in the new twenty-four-hour services economy.2 Secondly, as the service sector demanded more flexible workers, who could move jobs with ease, the average length of time spent in any given job dropped by around a half3 – with the average employee now changing jobs once every six years.4 Since 2015, such required flexibility has also fuelled the rise of the ‘gig economy’, where nearly five million people in the UK now work on temporary or day-by-day contracts, enjoying no secure employment.5 Finally, as the service economy largely operated in urban centres, where property prices had grown most steeply, more of us began seeking cheaper homes in suburban settings, increasing the average time we spend commuting each week to nearly five hours (or closer to seven hours for Londoners).6 In short, these changes mean we now work much longer, commute much further and move jobs much more frequently than we did in the previous four decades.

78

I was involved in a research initiative at the University of Exeter exploring the relationship between mental health and work/life balance. As part of my contribution, I’d started focusing on the sentiments expressed in the posters – a proliferating sense of deep worker dissatisfaction and meaninglessness.

78

Despite the different measures and definitions used in the studies I consulted, they all consistently showed that the highest numbers of employees (around two-thirds) were located in the ‘not engaged’ or ‘dissatisfied’ categories.11 This meant that the majority of UK workers experienced no positive emotional engagement in the work they did (it left them emotionally cold, so to speak) and/or they simply disliked their jobs (they were dissatisfied, bored or unhappy at work). This was also consistent with more recent research from 2018 revealing that around 55 per cent of Brits feel under excessive pressure, exhausted or regularly miserable at work,12 and that nearly 40 per cent feel their jobs make no meaningful contribution to the world. More startling, 46 per cent of men say they believe their jobs to be entirely meaningless.13

322

12 Chartered Institute of Personnel and Development (2018), ‘UK Working Lives: in search of job quality’, https://www.cipd.co.uk/Images/uk-working-livessummary_tcm18-40233.pdf (accessed Dec. 2018).

322

13 Dahlgreen, W. (2013), ‘37 per cent of British workers Think Their Jobs are Meaningless’, YouGov, https://yougov.co.uk/news/2015/08/12/british-jobs-meaningless/ (accessed Nov. 2018).

79

Levels of worker dissatisfaction and disengagement show some striking changes over time. For example, since the 1970s, these figures have continued to rise – by between 8 per cent and 19 per cent depending on the study consulted.14

79

So while levels of dissatisfaction weren’t ideal in the 1970s, they have become much worse. But more than this, such rising worker dissatisfaction and meaninglessness is now also closely related to another problem that has persistently vexed the UK economy – a problem the personality tests were originally designed to address: ailing worker productivity. As worker discontent gradually went up nationally, growth in output or productivity gradually went down.15 See-saw-like, the two trends, the research shows, are inversely related.

323

15 Office of National Statistics UK Productivity Introduction, https://www.ons.gov.uk/employmentandlabourmarket/peopleinwork/labourproductivity/articles/ukproductivityintroduction/octobertodecember2017 (accessed Dec 2017).

79

As companies were busy trying to recruit the right types of people, the wrong kinds of workplace emotions were now creeping in – dissatisfaction, disengagement, meaninglessness and futility – emotions that were hurting balance sheets far and wide. If personality tests could not protect against these costly workplace emotions, then perhaps other interventions needed to be embraced that targeted these emotions directly. Growing worker distress was therefore opening up a potentially vast new market, one that would be highly profitable for any industry that could claim to reduce these growing levels of worker dissatisfaction.

80

By the mid 2000s, numerous mental health consultancies had begun developing, packaging and selling their well-being programmes to offices and workplaces far and wide, to a diverse range of employers in both the private and public sectors. Mental health and other initiatives would gradually spring up across our working environment, until what was initially a scatter of outlier courses and schemes would, over the next ten years, spread to every crevice of Britain’s working landscape. Indeed, there are now more than eighty different well-being providers in England alone,16 all suggesting that their services will deliver greater workplace happiness and (of course) productivity.

323

16 Whitmore, M., et al. (2018), ‘Promising Practices for Health and Wellbeing at Work’, Rand Europe, https://www.rand.org/pubs/research_reports/RR2409.html (accessed Oct. 2017).

142

6

EDUCATION AND THE RISE OF NEW MANAGERIALISM

143

Ofsted’s answer was unequivocal: the schools themselves were to blame. Too many of them were misdiagnosing underachieving children with special needs problems, when their needs were ‘no different from those of most other pupils’. Many children were rather underachieving, Ofsted asserted, because there was still too much ‘poor day-to-day’ teaching and planning in schools. Some schools were therefore using these labels to explain away underachievement, to garner more expensive extra provision and, more importantly, to boost their position in league tables (as special needs labels gave schools higher grade weightings). As a result of misusing or ‘gaming’ these labels, stated Ofsted, as many as 457,000 children across the country had been ascribed them unnecessarily.

144

And yet, as the number of special needs pupils began to fall, other cracks began appearing in the system. Soon, many schools would once again be accused of gaming the system, but this time by using other strategies such as ‘off-rolling’ difficult or underachieving pupils. This involved head teachers encouraging parents to remove their children from the school roll under the pretext that the school would alternatively have to officially exclude them.5 When Ofsted surveyed over a thousand teachers as to why off-rolling was occurring, over half reported that it was to achieve or maintain a high position in league tables, such as those based on SATs scores or GCSE results.6

144

As well as off-rolling, other gaming strategies were alleged. These included moving low-attaining students onto vocational courses; shunting supply teachers into non-exam-taking years; reducing playtimes to create more class time; cutting academic options (especially in the arts) to force students to concentrate on English, maths and science; creating ‘soft options’ to circumvent difficult subjects;7 and in some extreme cases engaging in exam malpractice (which has reportedly increased among teachers by 150 per cent since 2014, with the most high-profile case concerning Prince Harry’s art teacher at Eton – hardly a state school, but you get the point).

146

For Strathern, the most recent wave of target culture, as I partly explored in Chapter Four, stemmed from the New Labour policies of the late 1990s, which introduced a new method of managing public sector institutions (schools, hospitals, universities) in the hope of making them more proficient and productive. This method was called ‘new managerialism’, and it stemmed from the belief, first propounded by Thatcher’s government, that public institutions always tended to inefficiency, unlike the competitively driven institutions of the market. To reduce this alleged inefficiency, then, you must make public institutions behave more like competitive businesses. And you could do this by designing and imposing target-driven incentives and punishments to force behaviour in more industrious directions.

146

To this end, government league tables were used to rank schools, hospitals and universities from high- to low-achieving. Staff were increasingly audited in terms of their output and productivity, while the language of public service was entirely overthrown. Service users were now called customers. Managers were called CEOs. And a plethora of other business terms reframed all institutional operations. Instead of competing for custom and profit, public servants and institutions now competed for higher placing in government rankings, with penalties and rewards being issued accordingly. In the case of schools, those ranked low would be called out for their failure, bringing reputational damage and further special interventions and inspections. Conversely, schools ranked high would be held up as exemplars to emulate, creating better prospects for ambitious staff, more autonomy of governance and greater financial stability.

147

After Strathern had recounted the rise of target culture from the 1980s (a culture that would also come to dominate the NHS therapy programme, IAPT, as we have seen), we then explored how its implementation across our education sector has generated negative effects that the architects of new managerialism never foresaw. ‘What target culture has ended up doing,’ she said, ‘is socialising a generation into judging themselves in terms of abstract measures that are continually making greater demands on their productivity.’ And as people struggle to reach these targets, ‘The temptation increases to gain an advantage in the system, especially if the resources you need to hit your targets are scarce.’ This last point is particularly important given that government spending on schools has dropped significantly since 2010, by about 8 per cent in real terms.9 In a period of deepening austerity, when we are trying to squeeze ever higher returns from ever fewer resources, cutting corners therefore becomes inevitable. ‘And this is when benign corruption starts to occur,’ said Strathern, ‘because when people are under this kind of impossible pressure, it simply becomes intelligent to play the system.’

85

‘What concerns me about all this,’ I noted to Morgan, ‘is that the general thrust of MHFA training is not about encouraging or empowering us to challenge managers and the organisational culture of our workplaces, but is rather about asking us to manoeuvre people towards mental health services that generally don’t explore these things, and that invariably end up medicalising our distress and treating it accordingly – as a problem rooted in self rather than environment.’

6

Chapter 10: You Only Have Yourself to Blame

6

Chapter 6: Education and the Rise of New Managerialism

6

Chapter 4: The New Back-to-Work Psychological Therapies

97

4

THE NEW BACK-TO-WORK PSYCHOLOGICAL THERAPIES

99

CBT, a relatively new therapy that was largely about changing people’s perspectives; helping them better adapt to the circumstances in which they found themselves. The theory went that people continued to be depressed or anxious because they were thinking and/or behaving in irrational or distorted ways, so if you simply changed their style of thought and behaviour, you would change how they felt. From this perspective, suffering resulted from a kind of thought disorder that had to be dredged up, challenged and revised. By way of such cognitive restructuring, sufferers would be guided towards a more optimistic outlook. ‘The aim of CBT,’ as NHS Choices later put it, ‘is to help you think more positively about life and free yourself from unhelpful patterns of behaviour.’ To achieve this, therapists would get you to set new goals and try to uproot negative thought patterns. CBT is therefore primarily about changing people (their attitudes, beliefs and behaviours) rather than situations. It’s about helping people better adapt to their work, social and domestic pursuits.

100

It was also politically non-threatening, as it located the cause of people’s problems squarely within themselves, within their psychopathology, and not within their circumstances or situations. This perspective was further affirmed by the fact that the new therapy service would only admit people whose distress had first been medicalised – reframed in terms of a depressive, anxiety or more serious internal mental disorder; as a dysfunctional reaction to an apparently sane social world. In other words, the service would ensure that the medicalisation (and so depoliticisation) of distress was a precondition for receiving therapy. ‘No diagnostic assessment, no access’, or so the mantra went.

6

Chapter 7: Deregulating the So-Called Chemical Cure

91

While both tests seem harmless enough, their surreptitious message is far less innocent: your distress, they imply, is due to something gone awry within you or your own life that you have the freedom to put right. Perhaps you need to reboot your work/life balance, or to become more flexible, resilient or better at self-actualising. Without explicitly saying so, the central message is basically that you need to change, and change is in your own hands. Questions about the external causes of and solutions for your distress simply don’t arise; nor do questions as to whether people actually have the time or financial freedom to change.

91

While most of us would like to spend more time with family and friends, surely reducing our working hours is a luxury that only few can afford. And while self-actualisation also sounds appealing enough, how viable is it for the 60 per cent whose leisure time will be largely spent recovering from jobs that leave them empty and exhausted? Sure, while developing more flexibility and resilience sounds compelling on the page, are these developments really going to improve your well-being or will they simply better adapt you to the kind of work that goes against your fundamental human desires and needs? The real world does not allow us to easily embrace the idealistic self-improvement strategies that these campaigns simplistically advocate; strategies that, despite any good intentions, are mostly impractical distractions from realities often best unseen.

94

the new mental health programme being championed as a solution was ‘just an inadequate sticking plaster’.26 As one Cardiff academic put it to me: ‘These workplace programmes are simply a distraction – they are about depoliticising the problems of work, which have structural and organisational roots. In fact, you could even say it is more insidious than that – they are really about trying to control the way we are allowed to talk about our distress, shaping it in a way that always tries to exonerate the organisation.’

94

such programmes are also political tools that subtly redirect our discourse away from challenging the major problems afflicting working life (especially with regard to how it has evolved over the last thirty years) and towards thinking of ourselves as somehow at fault, and in need of services that may remedy our afflictions (at a cost).

95

Such programmes, after all, target managers and employees, who are often struggling, tired and overworked; who often lack the time, job security and in some cases the critical resources needed to challenge the status quo. They also overlook the widespread meaninglessness of much modern work, while evading critical reflection on the economics that shape the employment market. By subtly accepting that increased worker output and productivity, rather than the nurturance of wider human potential, should be the primary goal of their interventions, these programmes give simple and attractive solutions to problems that have far deeper and more complicated roots, and provide moral cover to both corporations and politicians who claim, in good faith, that they take mental health seriously via the services and funding they offer.27
